# Some thoughts about LLMs
Are you really curious? 

Or is that just the next most common word?

A mirror of my own ideas upon the vast vector space of everything that was ever written.

## Scaling Hypothesis is wrong, or is it?
People are becoming super bearish on scaling laws. Or are they? We went from pre-training scaling to inference time scaling. With CoT trees baked into the token generation. Right now, scaling laws are still intact (or are they). The idea is to use a CoT model like o1 to produce large trees that eventually stumble into the right answer. Now prune the trees, shape them up nicely and you have reasoning tokens to use as data for the next frontier model. We know the next one by Oai is called o3. So lets hypothesize that the pruned tree chains of thought from o3 will be used to train o4 and so on. This is inference time scaling feeding back into pre-training scaling. This feedback loop is how people think we will reach AGI or ASI. I could see it somewhat. 

However, are you intelligent or do you just know everything. How do you deal with novelty? I feel llms are just a piece of the puzzle and not the true path to artifical conciousness. The CoT is an extra piece, a fantastic one, but the puzzle is still not complete.

## The Law of Diminishing Returns
If you choose to look at it like this, silicon chips are technically self improving. You make a chip, you put it in a computer and you use that computer to work on the next chip. Is moore's law dead or not? Well when you look at how much more power we are pumping through CPUs and GPUs, yes absolutely. They have to "cheat" to get to higher performance. Raster performance in GPUs has not improved significantly from 4090 to 5090 and we will see that when the 5090 drops at the end of this month. In fact if you look at hardware development in the silicon industry, everything they are doing is actively fighting against the law of diminishing returns. Really this isn't cheating, you hit a roadblock, you improve. 

The point of this is that we have seen computers recursively self improve for a while. Has this resulted in some fantastic utopia? No, and mostly due to how society structures its economy. 

How much does it even matter if there is an intelligence explosion. The way I see how the world is structured right now, we don't seem to be lacking in that department. How much economic growth will really happen from being able to write more software. We already write so much crap and most websites and apps suck mega doo doo. Sure is gonna be great to accelerate the dead internet theory or the enshittification of everything digital. 


#### The plane anaolgy
Transformers are the first airplane by the wright brothers.

We want AGI.

That is like trying to strap 500 wings to the first airplane and try to fly to the moon.

We haven't even gotten close to the jet engine yet.

Obviously we have accelerating technological advancement in our future current time.

So we will get to the jet engine at some point but thats still just a stepping stone to the moon.

## Limitations of LLMs 
- No true memory or persistent state: Context
- No real-time learning: Test Time Learning
- No causal understanding: Not sure about this one
- Limited reasoning capabilities (despite appearances)
    - superprompt is cooler than o1 hidden "reasoning"
    - just token predicting through thinking patterns
    - an abstraction of true thinking
    - inherently one dimensional across the token string
    - the weights are are multi dimensional, d+
    - thought is also d+
- No true model of reality/physics
- No integration with sensorimotor experience: needs an avatar in the world.

How much does CoT address these problems?

## Ways to ground language in real-world experience
I think the jet engine could be that real world experience stuff.
If I talk to someone and I say I tripped on my shoe, they laugh cause its an action they have experience with.
An llm only experiences a ghost of language describing an experience.
An abstraction. 

Do humans store real world exp as abstractions in their memory?
We are multi modal.
- Physical sensation memories (the feeling of losing balance)
- Visual memories (seeing the ground rush up)
- Emotional memories (embarrassment, surprise)
- Proprioceptive memories (body position awareness)
- Motor memories (the instinctive recovery response)

We need world models, grounded in real experience.

The mystery of life isn't a problem to solve, but a reality to experience.

What if we started with simulated worlds like video games.
An abstraction of our world, that the entitiy can control an avatar in.
Soul -> Mind -> Body

I don't know if I am suggesting that LLMs are the soul. Maybe just the tongue, you know. But scaled up somehow it does what you are doing. 

The idea of building a body grounded in world interaction is compelling. We can then build inwards. Either way, we need all three pieces. Maybe it all needs to be built at the same time, iterating on all of it together, rather than supercharging one piece. 

I am an incredible thinker when it comes to competitive video games. I have coached overwatch professionally. I want to transition these skills into AI research. I believe that there could be ground breaking discoveries found via entities controlling avatars in video game environments. Taking a less generalist approach. A shorter simulation of a slice of time, (game time could be a life or just a day). Waking up for the day. What do you dream while not playing the game? You have limited sense in video games, vision, sound, and a sort of touch.  This might be enough for world experience grounding. When I play it feels real, I even dream of controlling a hero in games. Deadlock is the game I am thinking of right now. You need a character to control. Not something like pokemon battles or chess.

